{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN and test it out with the Kaggle Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import hist\n",
    "from keras.preprocessing import sequence\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest phrase: 265\n",
      "\n",
      "\n",
      "                                              Phrase  Sentiment\n",
      "0  a series of escapades demonstrating the adage ...          1\n",
      "1  a series of escapades demonstrating the adage ...          2\n",
      "2                                           a series          2\n",
      "3                                                  a          2\n",
      "4                                             series          2\n",
      "5  of escapades demonstrating the adage that what...          2\n",
      "6                                                 of          2\n",
      "7  escapades demonstrating the adage that what is...          2\n",
      "8                                          escapades          2\n",
      "9  demonstrating the adage that what is good for ...          2\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the data.\n",
    "movie_reviews = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "movie_reviews = movie_reviews[['Phrase', 'Sentiment']]\n",
    "movie_reviews['Phrase'] = movie_reviews['Phrase'].str.lower()\n",
    "movie_reviews['Phrase'] = movie_reviews['Phrase'].str.replace(r'[^a-z ]*', '', regex=True)\n",
    "\n",
    "print(\"Largest phrase: {}\".format(movie_reviews['Phrase'].str.len().max()))\n",
    "print(\"\\n\")\n",
    "print(movie_reviews[0:10].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary to id mapping.\n",
    "stop_words = ['the', 'a', 'of', 'and', 'to', 's', 'in', 'is', 'its', 'an', 'be', 'by', 'at', 'rrb', 'lrb', 'i', \n",
    "              'you' 'it', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "              \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "              'she', \"she's\", 'her', 'hers', 'herself']\n",
    "\n",
    "phrases = movie_reviews['Phrase']\n",
    "review_corpus = ' '.join(phrases.tolist())\n",
    "tokens = nltk.word_tokenize(review_corpus)\n",
    "words = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "int_to_vocab = dict(enumerate(set(words)))\n",
    "vocab_to_int = dict(zip(int_to_vocab.values(), int_to_vocab.keys()))\n",
    "\n",
    "def verify_lookups(i2v, v2i):\n",
    "    for k in i2v:\n",
    "        v = i2v[k]\n",
    "        if k != v2i[v]:\n",
    "            print('not matching, {0}'.format(v))\n",
    "    print('works')\n",
    "\n",
    "verify_lookups(int_to_vocab, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lookup functions\n",
    "def get_vocab_id(word):\n",
    "    return vocab_to_int.get(word, None)\n",
    "\n",
    "def get_id_vocab(id):\n",
    "    return int_to_vocab.get(id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training set: 124848 \n",
      "Length of Validation set: 31212\n"
     ]
    }
   ],
   "source": [
    "#Divide into train, validate, and validation sets\n",
    "train, validate =  np.split(movie_reviews.sample(frac=1), [int(.8*len(movie_reviews))])\n",
    "\n",
    "train_x, train_y = train['Phrase'], train['Sentiment']\n",
    "validate_x, validate_y = validate['Phrase'], validate['Sentiment']\n",
    "\n",
    "print(\"Lenght of training set: {0} \\nLength of Validation set: {1}\"\n",
    "      .format(len(train), len(validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1456.,     0.,  5475.,     0.,     0., 15914.,     0.,  6591.,\n",
       "            0.,  1776.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just to make sure we didn't mess up the distributions print out the sentiment distributions of the sets.\n",
    "train_distribution = train.groupby(by='Sentiment').count().reset_index()\n",
    "validate_distribution = validate.groupby(by='Sentiment').count().reset_index()\n",
    "\n",
    "hist(train_distribution['Sentiment'], weights=train_distribution['Phrase'])\n",
    "hist(validate_distribution['Sentiment'], weights=validate_distribution['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHRASE IN TEXT: a series of escapades demonstrating the adage that what is good for the goose is also good for the gander  some of which occasionally amuses but none of which amounts to much of a story \n",
      "CONVERTED PHRASE: [11149, 3523, 594, 2876, 11756, 5968, 10252, 15947, 6486, 13959, 10252, 15947, 12513, 4221, 6171, 10103, 10605, 6665, 15138, 6171, 12117, 4507, 833]\n",
      "\n",
      "Before\n",
      "38855                  overcome his personal obstacles and\n",
      "89638    combining heated sexuality with a haunting sen...\n",
      "Name: Phrase, dtype: object\n",
      "\n",
      "After\n",
      "38855                              [1126, 9451, 8957]\n",
      "89638    [11450, 13176, 15438, 14855, 7189, 73, 5313]\n",
      "Name: Phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Functions to convert the values to their ids.\n",
    "def convert_vocab_to_id(phrase):\n",
    "    phrase_ids = []\n",
    "    for w in phrase.split(' '):\n",
    "        id = vocab_to_int.get(w, None)\n",
    "        if id:\n",
    "            phrase_ids.append(id)\n",
    "        else:\n",
    "            continue\n",
    "    return phrase_ids\n",
    "\n",
    "print(\"PHRASE IN TEXT: {}\".format(movie_reviews['Phrase'][0]))\n",
    "print(\"CONVERTED PHRASE: {}\".format(convert_vocab_to_id(movie_reviews['Phrase'][0])))\n",
    "\n",
    "train_X = train_x.map(lambda phrase: convert_vocab_to_id(phrase))\n",
    "validate_X = validate_x.map(lambda phrase: convert_vocab_to_id(phrase))\n",
    "\n",
    "print(\"\\nBefore\")\n",
    "print(train_x[:2])\n",
    "print(\"\\nAfter\")\n",
    "print(train_X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 0, 0, 0, 1], 1: [0, 0, 0, 1, 0], 2: [0, 0, 1, 0, 0], 3: [1, 0, 0, 0, 0], 4: [0, 1, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# One hot code the targets.\n",
    "hot_encoded_targets = pd.get_dummies(train_y.unique()).to_dict()\n",
    "targets_mapping = {k:list(v.values()) for k, v in hot_encoded_targets.items()}\n",
    "print(targets_mapping)\n",
    "\n",
    "Y_train = train_y.map(lambda target: targets_mapping[target]).tolist()\n",
    "Y_validate = validate_y.map(lambda target: targets_mapping[target]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "max_phrase_size = movie_reviews['Phrase'].str.len().max()\n",
    "vocabulary_size = len(int_to_vocab)\n",
    "embedding_size = 256\n",
    "batch_size = 256\n",
    "num_of_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 265, 256)          4141056   \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 265, 256)          525312    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,864,133\n",
      "Trainable params: 4,864,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_phrase_size, name='embedding_layer'))\n",
    "\n",
    "model.add(LSTM(256,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128,dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "\n",
    "model.add(Dense(5, activation='sigmoid', name='dense_layer'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/5\n",
      " 34048/124848 [=======>......................] - ETA: 45:34 - loss: 1.2730 - acc: 0.5079"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-2a282af2960b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_of_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     callbacks=callbacks, verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/movie_reviews_sentiment_analysis/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/movie_reviews_sentiment_analysis/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/movie_reviews_sentiment_analysis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/movie_reviews_sentiment_analysis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/movie_reviews_sentiment_analysis/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "X_train = sequence.pad_sequences(train_X, maxlen=max_phrase_size)\n",
    "X_validate = sequence.pad_sequences(validate_X, maxlen=max_phrase_size)\n",
    "\n",
    "#save weigths after training\n",
    "checkpoint = ModelCheckpoint('./lstm_weights.best.hdf5', monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, np.array(Y_train),\n",
    "                    validation_data=(X_validate,  np.array(Y_validate)),\n",
    "                    batch_size=batch_size, epochs=num_of_epochs,\n",
    "                    callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
